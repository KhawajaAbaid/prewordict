{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-06-15T17:19:26.179061Z","iopub.status.busy":"2022-06-15T17:19:26.178174Z","iopub.status.idle":"2022-06-15T17:19:31.98013Z","shell.execute_reply":"2022-06-15T17:19:31.97933Z","shell.execute_reply.started":"2022-06-15T17:19:26.178624Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from numpy.linalg import norm\n","from sklearn.preprocessing import normalize\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from wordcloud import WordCloud\n","import os\n","import shutil\n","import pathlib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T17:19:34.246594Z","iopub.status.busy":"2022-06-15T17:19:34.245946Z","iopub.status.idle":"2022-06-15T17:19:34.253295Z","shell.execute_reply":"2022-06-15T17:19:34.252377Z","shell.execute_reply.started":"2022-06-15T17:19:34.246553Z"},"trusted":true},"outputs":[],"source":["base_dir = pathlib.Path(\"/kaggle/working/output/\")\n","os.makedirs(base_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T13:15:41.239515Z","iopub.status.busy":"2022-06-15T13:15:41.239081Z","iopub.status.idle":"2022-06-15T13:15:41.244214Z","shell.execute_reply":"2022-06-15T13:15:41.243042Z","shell.execute_reply.started":"2022-06-15T13:15:41.239479Z"},"trusted":true},"outputs":[],"source":["# array = np.arange(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T13:35:26.003463Z","iopub.status.busy":"2022-06-15T13:35:26.003039Z","iopub.status.idle":"2022-06-15T13:35:26.037739Z","shell.execute_reply":"2022-06-15T13:35:26.03584Z","shell.execute_reply.started":"2022-06-15T13:35:26.003419Z"},"trusted":true},"outputs":[],"source":["# dataset = tf.data.Dataset.from_tensor_slices(array)\n","# dataset = dataset.batch(10)\n","# for i, batch in enumerate(dataset):\n","#     print(f\"{i}: {batch.numpy()}\")\n","# print(len(dataset))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:22:02.3069Z","iopub.status.idle":"2022-06-15T13:22:02.308017Z","shell.execute_reply":"2022-06-15T13:22:02.307724Z","shell.execute_reply.started":"2022-06-15T13:22:02.307695Z"},"trusted":true},"outputs":[],"source":["# all_batches = []\n","# for i in range(10):\n","#     batch = np.random.choice(array, size=10, replace=False)\n","#     all_batches.extend(batch)\n","#     print(f\"{i}th iter: {batch}\")\n","# print(f\"Total unique elements sampled: {len(np.unique(np.array(all_batches), return_counts=False))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T13:32:26.881657Z","iopub.status.busy":"2022-06-15T13:32:26.881195Z","iopub.status.idle":"2022-06-15T13:32:47.521753Z","shell.execute_reply":"2022-06-15T13:32:47.492908Z","shell.execute_reply.started":"2022-06-15T13:32:26.881622Z"},"trusted":true},"outputs":[],"source":["articles_dataset = tf.data.Dataset.from_tensor_slices((pd.read_csv(\"/kaggle/input/medium-articles/medium_articles.csv\")['text']).values)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T13:35:10.787812Z","iopub.status.busy":"2022-06-15T13:35:10.787384Z","iopub.status.idle":"2022-06-15T13:35:10.794337Z","shell.execute_reply":"2022-06-15T13:35:10.793332Z","shell.execute_reply.started":"2022-06-15T13:35:10.787778Z"},"trusted":true},"outputs":[],"source":["articles_dataset = articles_dataset.batch(9000)\n","print(len(articles_dataset))"]},{"cell_type":"markdown","metadata":{},"source":["## We've got 22 batches of length 9000 each but we have about 190K or 190K+ artciles so it's obvious that the last batch would be around ~1K texts, which may not be suitable for us, but generate word clouds from it anyway."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T13:48:30.210258Z","iopub.status.busy":"2022-06-15T13:48:30.209788Z","iopub.status.idle":"2022-06-15T13:48:30.21681Z","shell.execute_reply":"2022-06-15T13:48:30.215736Z","shell.execute_reply.started":"2022-06-15T13:48:30.210219Z"},"trusted":true},"outputs":[],"source":["def rank_words_by_tfidf(indices, words_list):\n","    \"\"\"Ranks the words, specified by indices which are sent in by \"cluster_to_cloud\"\n","    function. Ranking is based on the summed tfidf score \"\"\"\n","    \n","    summed_tfidf = np.asarray(tfidf_matrix[indices].sum(axis=0))\n","    data = {\"Words\": words_list,\n","           \"Summed_TFIDF\": summed_tfidf}\n","    return pd.DataFrame(data).sort_values(\"Summed_TFIDF\", ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-15T13:51:21.431826Z","iopub.status.busy":"2022-06-15T13:51:21.431417Z","iopub.status.idle":"2022-06-15T13:51:21.439803Z","shell.execute_reply":"2022-06-15T13:51:21.438874Z","shell.execute_reply.started":"2022-06-15T13:51:21.431795Z"},"trusted":true},"outputs":[],"source":["def cluster_to_cloud(df_cluster, max_words=15, cluster_num=0, batch_num=0):\n","    \"\"\"Generates a word cloud image using the top 15 words \n","    (which are ranked by their tfidf score) in the given cluster\"\"\"\n","    \n","    indices = df_cluster.Index.values\n","    df_ranked_words_all = rank_words_by_tfidf(indices)\n","    df_ranked_words_in_cloud = df_ranked_words_all[:max_words]\n","    df_ranked_words_remaining = df_ranked_words_all[max_words:]\n","    \n","    df_ranked_words_remaining.to_csv(f\"cluster_{cluster_num}_words_batch_{batch_num}.csv\")\n","    words_to_score = {word:score\n","                     for word, score in df_ranked_words_in_cloud.values}    \n","    cloud_generator = WordCloud(background_color=\"white\",\n","                               random_state=1, width=2000, height=1000)\n","    wordcloud_image = cloud_generator.fit_words(words_to_score)\n","    return wordcloud_image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def vectorizeX(batch, batch_num=0):\n","    \"\"\"Vectorizes the texts,generates the TFIDF matrix\n","    and returns TFIDF matrix, words, and sorted words TFIDF dataframe.\n","    X in the function names stands for eXtra, as it performs and returns some extra things\n","    also in all honesty, it's more because it's sounds cool this way. No judging, okay?\"\"\"\n","    \n","    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(batch).toarray()\n","    words = tfidf_vectorizer.get_feature_names()\n","    words_tfidf_df = pd.DataFrame({\"Words\": words, \"Summed_TFIDF\": tfidf_matrix.sum(axis=0)})\n","    sorted_words_tfidf_df = words_tfidf_df.sort_values(by=\"Summed_TFIDF\", ascending=False)\n","    \n","    return words, tfidf_matrix, sorted_words_tfidf_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clusterItUp(batch, batch_num=0):\n","    \"\"\"Takes the normalized shrunk matrix of a batch and clusters the words using KMeans\n","    and returns the dataframe with each word assigned a relevant cluster id\"\"\"\n","     \n","    #as mentioned earlier, since our last batch would contain just around 1K texts hence it would have less\n","    # words and less diversity. So we cluster that in a 3 groups instead of 30 which by the way is the number of groups\n","    # we found in our ealier extensive testing and prototyping.\n","          \n","    print(\"/tClustering words into groups...\")\n","    if batch_num != 21:\n","        cluster_model = KMeans(n_clusters=30)\n","        clusters = cluster_model.fit_predict(shrunk_norm_matrix)\n","        # we are using Z to make the plural in clusters, more apparent and more easily distinguishable\n","        clusterZ_df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})\n","    else:\n","        cluster_model = KMeans(n_clusters=3)\n","        clusters = cluster_model.fit_predict(shrunk_norm_matrix)\n","        clusterZ_df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})\n","    print(\"\\t\\tSuccessfully Clustered!\")\n","    return clusters_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for batch_num, batch in enumerate(artciles_dataset):\n","    \n","    print(f\"\\nProcessing batch {batch_num}...\")\n","    \n","    # TODO: i- implement zipping the whole base output folder\n","    #       ii- save vectorize matrix for each cluster in a file to later read that in\n","    #           browser and use that to compute the user entered word's similarity with the cluster\n","    # now that i think about it, there's no way to calculate similarity with mere dot product between\n","    # a single word vector and vector of all the words in the given space or word cloud.\n","    # i mean sure, we can measure similarity between two sentences but apply the same method for a\n","    # sentence and a word doesn't seem like would work.\n","    \n","    # make directories to neatly organize our output files\n","    # also would be a lot easier to download later\n","    batch_dir = base_dir / f\"batch{batch_num}\"\n","    cluster_words_dir = batch_dir / \"cluster_words\"\n","    cluster_clouds_dir = batch_dir / \"cluster_clouds\"\n","    os.makedirs(cluster_words_dir)\n","    os.makedirs(cluster_clouds_dir)\n","\n","\n","    print(f\"\\tTotal Words -> {len(words)}\"\")\n","    \n","    # calculate the words, tfidf_matrix and sorted words df by calling the vectorizeX function\n","    words, tfidf_matrix, sorted_words_tfidf_df = vectorizeX(batch, batch_num=batch_num)\n","    \n","    filename = batch_dir / f\"sorted_words_summed_tfidf_batch_{batch_num}.csv\"\n","    sorted_words_tfidf_df.to_csv(f\"{filename}\", index=False)\n","\n","    \n","    print(\"\\tApplying SVD...\")\n","    shrunk_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)\n","    # normalize the matrix\n","    shrunk_norm_matrix = normalize(shrunk_matrix)\n","    print(\"\\t\\tSuccessfully applied SVD.\")\n","    # norm(shrunk_norm_matrix[0])\n","    \n","          \n","    clusterZ_df = clusterItUp(shrunk_norm_matrix, batch_num=batch_num)\n","    # making a list of clustered groups for further manipulation\n","    cluster_groups = [df_cluster for _, df_cluster in clusterZ_df.groupby(\"Cluster\")]\n","#     len(cluster_groups)\n","    \n","    # the number of words we want in the word cloud image.\n","    # 15 seems to work well. But obviously you can change to whatever you want.\n","    max_words = 15\n","    \n","    # making a copy of clustered groups list so as to avoid accidently changing its elements\n","    cluster_groups_cp = cluster_groups[:]\n","    \n","    total_groups = len(cluster_groups_cp)\n","    \n","    print(f\"\\tGenerating word cloud images for clusters... \")\n","    for i in range(total_groups):\n","        cluster_df = cluster_groups_cp[i]\n","        wordcloud_image = cluster_to_cloud(cluster_df, cluster_num=i, batch_num=batch_num)\n","        wordcloud_image.to_file(f\"cluster_{i}_cloud.png\")\n","        \n","        if i+1%10==0:\n","          print(f\"\\t\\t{i+1} of {total_groups} word clouds generated...\")\n","    \n","    print(f\"\\tYay! Everything went well for batch {batch_num}. Onto the next one!\\n\")\n","    return"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"9b16f958606a7247fd2ba2750d9cad997c4b7c59ceec8c9b087db87db2a39d64"}}},"nbformat":4,"nbformat_minor":4}
