{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-06-17T05:01:44.046698Z","iopub.status.busy":"2022-06-17T05:01:44.046171Z","iopub.status.idle":"2022-06-17T05:01:50.127800Z","shell.execute_reply":"2022-06-17T05:01:50.126862Z","shell.execute_reply.started":"2022-06-17T05:01:44.046603Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from numpy.linalg import norm\n","from sklearn.preprocessing import normalize\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","from wordcloud import WordCloud\n","import os\n","import shutil\n","import pathlib\n","import gc\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-17T05:01:50.129978Z","iopub.status.busy":"2022-06-17T05:01:50.129396Z","iopub.status.idle":"2022-06-17T05:01:50.135381Z","shell.execute_reply":"2022-06-17T05:01:50.134467Z","shell.execute_reply.started":"2022-06-17T05:01:50.129947Z"},"trusted":true},"outputs":[],"source":["# root dir to store the zip file of the whole output folder which serves as the base directory\n","# for all the things output by the script\n","root_dir = pathlib.Path(\"/kaggle/working/\")\n","base_dir = pathlib.Path(\"/kaggle/working/output/\")\n","os.makedirs(base_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-17T05:01:55.042992Z","iopub.status.busy":"2022-06-17T05:01:55.041995Z","iopub.status.idle":"2022-06-17T05:02:29.104601Z","shell.execute_reply":"2022-06-17T05:02:29.103485Z","shell.execute_reply.started":"2022-06-17T05:01:55.042935Z"},"trusted":true},"outputs":[],"source":["articles_dataset = tf.data.Dataset.from_tensor_slices((pd.read_csv(\"/kaggle/input/medium-articles/medium_articles.csv\")['text']).values)\n","articles_dataset = articles_dataset.batch(8000)\n","total_batches = len(articles_dataset)\n","print(total_batches)"]},{"cell_type":"markdown","metadata":{},"source":["## We've got 25 batches of length 8000 each but we have about 190K or 190K+ artciles so the first 24 batches take up 192K articles hence it's obvious that the last batch would be around ~1K texts or less, which may not be suitable for us, but we generate word clouds from it anyway."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-17T05:02:42.194683Z","iopub.status.busy":"2022-06-17T05:02:42.194189Z","iopub.status.idle":"2022-06-17T05:47:23.718419Z","shell.execute_reply":"2022-06-17T05:47:23.717593Z","shell.execute_reply.started":"2022-06-17T05:02:42.194638Z"},"trusted":true},"outputs":[],"source":["# Collected all the code in one cell to save memory consumption as i was constantly running out of memory on kaggle when running SVD.\n","#------------------\n","\n","\n","def rank_words_by_tfidf(indices, words_list):\n","    \"\"\"Ranks the words, specified by indices which are sent in by \"cluster_to_cloud\"\n","    function. Ranking is based on the summed tfidf score \"\"\"\n","    \n","    summed_tfidf = np.asarray(tfidf_matrix[indices].sum(axis=0))\n","    data = {\"Words\": words_list,\n","           \"Summed_TFIDF\": summed_tfidf}\n","    return pd.DataFrame(data).sort_values(\"Summed_TFIDF\", ascending=False)\n","\n","\n","#-------------------\n","\n","\n","def cluster_to_cloud(df_cluster, max_words=15, cluster_num=0, words_list=None, batch_num=0, save_dir=None):\n","    \"\"\"Generates a word cloud image using the top 15 words \n","    (which are ranked by their tfidf score) in the given cluster\"\"\"\n","    \n","    indices = df_cluster.Index.values\n","    df_ranked_words_all = rank_words_by_tfidf(indices, words_list)\n","    df_ranked_words_in_cloud = df_ranked_words_all[:max_words]\n","    df_ranked_words_remaining = df_ranked_words_all[max_words:]\n","    \n","    filename = save_dir / f\"cluster_{cluster_num}_words_batch_{batch_num}.csv\"\n","    df_ranked_words_remaining.to_csv(filename, index=False)\n","    words_to_score = {word:score\n","                     for word, score in df_ranked_words_in_cloud.values}    \n","    cloud_generator = WordCloud(background_color=\"white\",\n","                               random_state=1, width=2000, height=1000)\n","    wordcloud_image = cloud_generator.fit_words(words_to_score)\n","    return wordcloud_image\n","\n","\n","#------------------\n","\n","\n","def vectorizeX(batch):\n","    \"\"\"Vectorizes the texts,generates the TFIDF matrix\n","    and returns TFIDF matrix, words, and sorted words TFIDF dataframe.\n","    X in the function names stands for eXtra, as it performs and returns some extra things\n","    also in all honesty, it's more because it's sounds cool this way. No judging, okay?\"\"\"\n","    \n","    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(batch).toarray()\n","    words = tfidf_vectorizer.get_feature_names_out()\n","    words_tfidf_df = pd.DataFrame({\"Words\": words, \"Summed_TFIDF\": tfidf_matrix.sum(axis=0)})\n","    sorted_words_tfidf_df = words_tfidf_df.sort_values(by=\"Summed_TFIDF\", ascending=False)\n","    \n","    return words, tfidf_matrix, words_tfidf_df, sorted_words_tfidf_df\n","\n","\n","#-------------------\n","\n","\n","def clusterItUp(shrunk_norm_matrix, batch_num=0):\n","    \"\"\"Takes the normalized shrunk matrix of a batch and clusters the words using KMeans\n","    and returns the dataframe with each word assigned a relevant cluster id\"\"\"\n","     \n","    #as mentioned earlier, since our last batch would contain just around 1K texts hence it would have less\n","    # words and less diversity. So we cluster that in a 2 groups instead of 15,\n","    # a number we found in our initial prototypes (that code is not included here)\n","          \n","    print(\"\\tClustering words into groups...\")\n","    if batch_num != 24:\n","        cluster_model = KMeans(n_clusters=15)\n","        clusters = cluster_model.fit_predict(shrunk_norm_matrix)\n","        # we are using Z to make the plural in clusters, more apparent and more easily distinguishable\n","        clusterZ_df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})\n","    else:\n","        cluster_model = KMeans(n_clusters=2)\n","        clusters = cluster_model.fit_predict(shrunk_norm_matrix)\n","        clusterZ_df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters})\n","    print(\"\\t\\tSuccessfully Clustered!\")\n","    return clusterZ_df\n","\n","# -------------\n","\n","for batch_num, batch in tqdm(enumerate(articles_dataset)):\n","    \n","    print(f\"\\nProcessing batch {batch_num+1} / {total_batches}\")\n","    \n","    # converts the tf object into numpy since we're accustomed ot numpy and pandas workflow\n","    batch = batch.numpy()\n","    \n","    # make directories to neatly organize our output files\n","    # also would be a lot easier to download later\n","    print(\"\\tMaking directories...\")\n","    batch_dir = base_dir / f\"batch{batch_num}\"\n","    cluster_words_dir = batch_dir / \"cluster_words\"\n","    cluster_clouds_dir = batch_dir / \"cluster_clouds\"   \n","    os.makedirs(cluster_words_dir, exist_ok=True)\n","    os.makedirs(cluster_clouds_dir, exist_ok=True)\n","    print(\"\\t\\tSuccessfully created required directories.\")\n","    \n","    # calculate the words, tfidf_matrix and sorted words df by calling the vectorizeX function\n","    print(\"\\tPerforming vectorization x...\")\n","    words, tfidf_matrix, words_tfidf_df, sorted_words_tfidf_df = vectorizeX(batch)\n","    print(\"\\t\\tSuccessfully vectorized x.\")\n","    \n","    print(f\"\\tTotal Words -> {len(words)}\")\n","    \n","    filename = batch_dir / f\"words_summed_tfidf_batch_{batch_num}.csv\"\n","    print(f\"\\tSaving {filename} to disk...\")\n","    \n","    filename = batch_dir / f\"sorted_words_summed_tfidf_batch_{batch_num}.csv\"\n","    print(f\"\\tSaving {filename} to disk...\")\n","    sorted_words_tfidf_df.to_csv(f\"{filename}\", index=False)\n","\n","    \n","    print(\"\\tApplying SVD...\")\n","    shrunk_matrix = TruncatedSVD(n_components=100).fit_transform(tfidf_matrix)\n","    print(\"\\t\\tSuccessfully applied SVD.\")\n","    \n","    # normalize the matrix\n","    print(\"\\tNormalizing shrunk matrix...\")\n","    shrunk_norm_matrix = normalize(shrunk_matrix)\n","    # norm(shrunk_norm_matrix[0])\n","    print(\"\\t\\tTarget normalized.\")\n","    \n","\n","    clusterZ_df = clusterItUp(shrunk_norm_matrix, batch_num=batch_num)\n","\n","    # though clusters file and summed tfidf file would pretty much contain the same words\n","    # with ONLY DIFFERENCE being that this cluster file would associate each word with its\n","    #cluster id. but we are saving it anyway.\n","    filename = batch_dir / f\"clustered_groups_batch_{batch_num}.csv\"\n","    clusterZ_df.to_csv(filename, index=False)\n","    # making a list of clustered groups for further manipulation\n","    cluster_groups = [df_cluster for _, df_cluster in clusterZ_df.groupby(\"Cluster\")]\n","#     len(cluster_groups)\n","    \n","    # the number of words we want in the word cloud image.\n","    # 15 seems to work well. But obviously you can change to whatever you want.\n","    max_words = 15\n","    \n","    # making a copy of clustered groups list so as to avoid accidently changing its elements\n","    cluster_groups_cp = cluster_groups[:]\n","    \n","    total_groups = len(cluster_groups_cp)\n","    \n","    print(f\"\\tGenerating word cloud images for clusters... \")\n","    for i in tqdm(range(total_groups)):\n","        cluster_df = cluster_groups_cp[i]\n","        wordcloud_image = cluster_to_cloud(cluster_df, cluster_num=i, \n","                                           words_list=words, batch_num=batch_num, \n","                                           save_dir=cluster_words_dir)\n","        \n","        filename = cluster_clouds_dir / f\"cluster_{i}_cloud_batch_{batch_num}.png\"\n","        wordcloud_image.to_file(filename)\n","    \n","    print(f\"\\tYay! Everything went well for batch {batch_num}. Onto the next one!\\n\")\n","    \n","    print(\"\\tClearning up memory for next iteration so we dont run out of memory...\")\n","    del batch, words, tfidf_matrix, words_tfidf_df, sorted_words_tfidf_df, shrunk_matrix, clusterZ_df, cluster_groups, cluster_groups_cp\n","    gc.collect()\n","\n","\n","# Zip up the whole output folder\n","shutil.make_archive(root_dir / \"output\", \"zip\", base_dir)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
